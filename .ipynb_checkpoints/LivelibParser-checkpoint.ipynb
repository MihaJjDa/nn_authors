{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Only needed for Python 2\n",
    "import pickle\n",
    "from collections import defaultdict \n",
    "import json\n",
    "import codecs\n",
    "\n",
    "proxies = {\n",
    "    'HTTP': 'http://37.235.67.178:34450',\n",
    "    'HTTP': 'http://109.201.97.204:41258',\n",
    "    'HTTP': 'http://37.29.48.78:8080',\n",
    "    'HTTP': 'http://85.237.57.198:42118',\n",
    "    'HTTP': 'http://31.133.121.28:31315',\n",
    "    'HTTP': 'http://95.31.4.20:54618',\n",
    "}\n",
    "\n",
    "def store_parsing_stage(s):\n",
    "    with open('parsing_stage.pickle', 'wb') as f:\n",
    "        pickle.dump(s.parsing_stage, f)\n",
    "        \n",
    "def restore_parsing_stage():\n",
    "    with open('parsing_stage.pickle', 'rb') as f:\n",
    "        parsing_stage = pickle.load(f)\n",
    "        return parsing_stage\n",
    "    return {'row':0, 'page':1}\n",
    "\n",
    "def store_dict(s):\n",
    "    with codecs.open('reader_dict.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(dict(s.reader_dict), f, sort_keys=True,\n",
    "                  indent=4, separators=(',', ': '), ensure_ascii=False)\n",
    "        \n",
    "def restore_dict():\n",
    "    with codecs.open('reader_dict.json', 'r', encoding='utf-8') as f:\n",
    "        reader_dict = json.load(f)\n",
    "        return defaultdict(list, reader_dict)\n",
    "    return defaultdict(list)\n",
    "\n",
    "def store(s):\n",
    "    store_dict(s)\n",
    "    store_parsing_stage(s)\n",
    "\n",
    "\n",
    "def update_parsing_stage(s):\n",
    "    s.parsing_stage['page'] = 1\n",
    "    s.parsing_stage['row'] += 1\n",
    "\n",
    "class Serialized:\n",
    "    def deserialize(self):\n",
    "        self.parsing_stage = restore_parsing_stage()\n",
    "        self.reader_dict = restore_dict()\n",
    "        \n",
    "def make_readers_dict(readers):\n",
    "    rdict = {}\n",
    "    for idx, r in enumerate(readers.itertuples()):\n",
    "        rdict[r.ссылка] = r\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "len 25\n",
      "idx 1\n",
      "len 25\n",
      "idx 2\n",
      "len 25\n",
      "idx 3\n",
      "len 25\n",
      "idx 4\n",
      "len 25\n",
      "idx 5\n",
      "len 25\n",
      "idx 6\n",
      "len 25\n",
      "idx 7\n",
      "len 25\n",
      "idx 8\n",
      "len 25\n",
      "idx 9\n",
      "len 25\n",
      "idx 10\n",
      "len 25\n",
      "idx 11\n",
      "len 25\n",
      "idx 12\n",
      "len 25\n",
      "idx 13\n",
      "len 25\n",
      "idx 14\n",
      "len 25\n",
      "idx 15\n",
      "len 15\n",
      "idx 16\n",
      "len 25\n",
      "idx 17\n",
      "len 25\n",
      "idx 18\n",
      "len 22\n",
      "idx 19\n",
      "len 25\n",
      "idx 20\n",
      "len 25\n",
      "idx 21\n",
      "len 25\n",
      "idx 22\n",
      "len 25\n",
      "idx 23\n",
      "len 25\n",
      "idx 24\n",
      "len 25\n",
      "idx 25\n",
      "len 25\n",
      "idx 26\n",
      "len 25\n",
      "idx 27\n",
      "len 5\n",
      "idx 28\n",
      "idx 30\n",
      "len 25\n",
      "idx 31\n",
      "len 25\n",
      "idx 32\n",
      "len 25\n",
      "idx 33\n",
      "len 25\n",
      "idx 34\n",
      "len 6\n",
      "idx 35\n",
      "len 25\n",
      "idx 36\n",
      "len 25\n",
      "idx 37\n",
      "len 10\n",
      "idx 38\n",
      "len 25\n",
      "idx 39\n",
      "len 25\n",
      "idx 40\n",
      "len 25\n",
      "idx 41\n",
      "len 2\n",
      "idx 42\n",
      "len 13\n",
      "idx 43\n",
      "len 25\n",
      "idx 44\n",
      "len 25\n",
      "idx 45\n",
      "len 25\n",
      "idx 46\n",
      "len 2\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from re import sub \n",
    "\n",
    "def clear_sentence(s):\n",
    "    return sub(r'[\\.\\—\\?\\!\\n\\t\\,\\«\\»\\)\\(\\:\\/\\=]',' ',s)\n",
    "\n",
    "bad_words=['спойлер','свернуть']\n",
    "def preprocessed_text(s):\n",
    "    def erase_space_symbols(s):\n",
    "        return sub(r'[\\t\\n]',' ', s)\n",
    "    def erase_spec_symbols(s):\n",
    "        return sub(r'[\\>\\<\\_\\*]', '', s)\n",
    "    def erase_links(s):\n",
    "        return sub(r'\\[.{,70}\\]\\(.{,200}\\)', 'ССЫЛКА', s)\n",
    "    for w in bad_words:\n",
    "        s = s.replace(w,'')\n",
    "    s = erase_space_symbols(s)\n",
    "    s = erase_spec_symbols(s)\n",
    "    s = erase_links(s)\n",
    "    return s\n",
    "\n",
    "def parse_reader(reader, s):\n",
    "    url_base = 'https://www.livelib.ru/reader/' + reader.ссылка + '/reviews/~'\n",
    "    review_count = 0\n",
    "    for page_number in range(s.parsing_stage['page'], 2):\n",
    "        url = url_base + str(page_number)\n",
    "        req = requests.get(url)\n",
    "        if not req:\n",
    "            print('bad req ' + url)\n",
    "            sleep(11)\n",
    "            break\n",
    "        req.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(req.text, \"lxml\")\n",
    "        html = soup.findAll('div', itemprop='reviewBody')\n",
    "        response_len = len(html)\n",
    "        if not response_len:\n",
    "            if 'Капча' in req.text:\n",
    "                print(req.text)\n",
    "                store(s)\n",
    "                raise\n",
    "            else:\n",
    "                update_parsing_stage(s)\n",
    "                break\n",
    "        review_count += response_len\n",
    "        for idx, div in enumerate(html):\n",
    "            text = html2text.html2text(str(div))\n",
    "            text = preprocessed_text(text)\n",
    "            s.reader_dict[reader.ссылка].append(text)\n",
    "        print('len ' + str(len(s.reader_dict[reader.ссылка])))\n",
    "\n",
    "        s.parsing_stage['page'] += 1\n",
    "#         store(s)\n",
    "        sleep(11)\n",
    "    update_parsing_stage(s)\n",
    "\n",
    "\n",
    "def reset(s):\n",
    "    s.parsing_stage = {'row':0,'page':1}\n",
    "    store_parsing_stage(s)\n",
    "    s.reader_dict = defaultdict(list)\n",
    "    store_dict(s)\n",
    "    \n",
    "s = Serialized()\n",
    "reset(s)\n",
    "s.deserialize()\n",
    "\n",
    "readers = pd.read_csv('readers.csv')\n",
    "\n",
    "for idx, r in enumerate(readers.itertuples()):\n",
    "    if idx < s.parsing_stage['row']:\n",
    "        continue\n",
    "    print('idx ' + str(idx))\n",
    "    parse_reader(r,s)\n",
    "    if not idx % 5:\n",
    "        store(s)\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "# split corpus by tags\n",
    "\n",
    "rdict = make_readers_dict(readers)\n",
    "\n",
    "# TAGS\n",
    "genders = ['ж', 'м']\n",
    "years = ['<18', '18-27','28-40', '>40']\n",
    "level = ['-', 'учитель', 'писатель']\n",
    "mentality = ['-', 'гум', 'тех']\n",
    "\n",
    "\n",
    "def fill_file(rmatcher, fname):\n",
    "    with codecs.open('splitted_corpora/' + fname, 'w', encoding='utf-8') as f:\n",
    "        for i in range(0,25):\n",
    "            for key, reader in rdict.items():\n",
    "                if rmatcher(reader):\n",
    "                    reviews = s.reader_dict[key]\n",
    "                    if i < len(reviews):\n",
    "                        print(reviews[i], file=f)\n",
    "\n",
    "# matchers\n",
    "f_matcher = lambda x: x.Пол == 'ж'\n",
    "m_matcher = lambda x: x.Пол == 'м'        \n",
    "\n",
    "yl18_matcher = lambda x: x.Год > 2000\n",
    "y18_27_matcher = lambda x: 1991 <= x.Год <= 2000\n",
    "y28_40_matcher = lambda x: 1978 <= x.Год <= 1991\n",
    "yg40_matcher = lambda x: x.Год > 1978\n",
    "\n",
    "no_level_matcher = lambda x: x.Профессия == '-'\n",
    "teacher_matcher = lambda x:  x.Профессия == 'учитель'\n",
    "writer_matcher = lambda x:   x.Профессия == 'писатель'\n",
    "\n",
    "no_mentality_mantcher = lambda x: x.гумтех == 'без'\n",
    "humanist_matcher = lambda x: x.гумтех == 'гум'\n",
    "techie_matcher = lambda x:   x.гумтех == 'тех'\n",
    "\n",
    "tag_dict = {\n",
    "    'м': {\n",
    "        'file':'males.txt',\n",
    "        'matcher':m_matcher,\n",
    "        'value':[0]\n",
    "    },\n",
    "    'ж': {\n",
    "        'file':'femalies.txt',\n",
    "        'matcher':f_matcher,\n",
    "        'value':[1]\n",
    "    },\n",
    "    'моложе_18': {\n",
    "        'file':'younger_18.txt',\n",
    "        'matcher':yl18_matcher,\n",
    "        'value': [0,0,0,1]\n",
    "    },\n",
    "    '18_27': {\n",
    "        'file':'from_18_to_27.txt',\n",
    "        'matcher':y18_27_matcher,\n",
    "        'value':[0,0,1,0]\n",
    "    },\n",
    "    '28_40': {\n",
    "        'file':'from_27_to_40.txt',\n",
    "        'matcher':y28_40_matcher,\n",
    "        'value':[0,1,0,0]\n",
    "    },\n",
    "    'старше_40': {\n",
    "        'file':'older_41.txt',\n",
    "        'matcher':yg40_matcher,\n",
    "        'value':[1,0,0,0]\n",
    "    },\n",
    "    'неизв_проф': {\n",
    "        'file':'unknown_level.txt',\n",
    "        'matcher':no_level_matcher,\n",
    "        'value':[0,0,1]\n",
    "    },\n",
    "    'учитель': {\n",
    "        'file':'teachers.txt',\n",
    "        'matcher':teacher_matcher,\n",
    "        'value0':[0,1,0]\n",
    "    },\n",
    "    'писатель': {\n",
    "        'file':'writers.txt',\n",
    "        'matcher':writer_matcher,\n",
    "        'value':[1,0,0]\n",
    "    },\n",
    "    'неизв_склад': {\n",
    "        'file':'unknown_mentality.txt',\n",
    "        'matcher':no_mentality_mantcher,\n",
    "        'value':[0,0,1]\n",
    "    },\n",
    "    'гуманитарий': {\n",
    "        'file':'humanists.txt',\n",
    "        'matcher':humanist_matcher,\n",
    "        'value':[0,1,0]\n",
    "    },\n",
    "    'технарь': {\n",
    "        'file':'techies.txt',\n",
    "        'matcher':techie_matcher,\n",
    "        'value':[1,0,0]\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "# fill files\n",
    "for key, tag in tag_dict.items():\n",
    "    fill_file(tag['matcher'], tag['file'])\n",
    "\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
